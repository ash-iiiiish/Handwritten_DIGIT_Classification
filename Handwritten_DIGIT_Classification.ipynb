{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN9G/rX3qF3VO84aAkmsjQf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/129Ashish/Handwritten_DIGIT_Classification/blob/main/Handwritten_DIGIT_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HANDWRITTEN DIGIT CLASSIFICATION\n",
        "\n",
        "## MNIST/CIFAR"
      ],
      "metadata": {
        "id": "QbwbITC4DyjL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P3UbaOAhDcpP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as mlt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset , DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "pBSslyTXHBE1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45b56786",
        "outputId": "6d0538fd-57a9-43b4-ad25-ccaadd25932d"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Display the shapes of the loaded data\n",
        "print(\"Shape of x_train:\", x_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of x_test:\", x_test.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Shape of x_train: (60000, 28, 28)\n",
            "Shape of y_train: (60000,)\n",
            "Shape of x_test: (10000, 28, 28)\n",
            "Shape of y_test: (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e135dea5",
        "outputId": "7089f343-5c11-48bc-811c-974968c73d31"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Convert NumPy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "print(\"Shape of x_train_tensor:\", X_train_tensor.shape)\n",
        "print(\"Shape of y_train_tensor:\", y_train_tensor.shape)\n",
        "print(\"Shape of x_test_tensor:\", X_test_tensor.shape)\n",
        "print(\"Shape of y_test_tensor:\", y_test_tensor.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x_train_tensor: torch.Size([60000, 28, 28])\n",
            "Shape of y_train_tensor: torch.Size([60000])\n",
            "Shape of x_test_tensor: torch.Size([10000, 28, 28])\n",
            "Shape of y_test_tensor: torch.Size([10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device : {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUPaQ3-yFBDn",
        "outputId": "89cccd9a-6168-4a11-ba2c-c11c0c547e77"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device : cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET CLASS"
      ],
      "metadata": {
        "id": "pGjVQpMQEnfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self,features,labels):\n",
        "    self.features=features\n",
        "    self.labels=labels\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.features[idx],self.labels[idx]"
      ],
      "metadata": {
        "id": "MJ8rW8HbEIlR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(X_train_tensor,y_train_tensor)\n",
        "test_dataset = CustomDataset(X_test_tensor,y_test_tensor)"
      ],
      "metadata": {
        "id": "Lwe3mDCVFTPT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATALOADER CLASS"
      ],
      "metadata": {
        "id": "tKmnlGXVEplh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
        "test_loader = DataLoader(test_dataset,batch_size=32,shuffle=True)\n",
        "\n",
        "print(f\"length of train batch : {len(train_loader)}\")\n",
        "print(f\"length of test batch : {len(test_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-PzhKZuEraV",
        "outputId": "370dcef8-9572-47d1-b99d-f3c70e18977a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of train batch : 1875\n",
            "length of test batch : 313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEFINING A MODEL"
      ],
      "metadata": {
        "id": "otktn6DzEr3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining class for model\n",
        "class MyNN(nn.Module):\n",
        "\n",
        "  def __init__(self,input_dim,output_dim,num_hidden_layer,neuron_per_layer,dropout_rate):\n",
        "    super().__init__()\n",
        "    #architecture of our neural network\n",
        "    layer=[]#dynamic logic\n",
        "    for i in range(num_hidden_layer):\n",
        "      layer.append(nn.Linear(input_dim,neuron_per_layer))\n",
        "      layer.append(nn.BatchNorm1d(neuron_per_layer))\n",
        "      layer.append(nn.ReLU())\n",
        "      layer.append(nn.Dropout(dropout_rate))\n",
        "      input_dim =neuron_per_layer#input of second layer will be output of previous layer\n",
        "\n",
        "    layer.append(nn.Linear(neuron_per_layer,output_dim))\n",
        "\n",
        "    self.model=nn.Sequential(*layer) # *this symbol unpacks the list and make the layers individually accessed\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.model(x)"
      ],
      "metadata": {
        "id": "1cr_lUsBEuAA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate=0.01\n",
        "epochs = 100"
      ],
      "metadata": {
        "id": "Pi_LL7U3JaCs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyNN(1)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(),lr=learning_rate,weight_decay =1e-4)"
      ],
      "metadata": {
        "id": "JgV8VYeIJc4H"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OBJECTIVE FUNCTION"
      ],
      "metadata": {
        "id": "jjAeDAPJEudz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# objective function\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "  #next hyperparameter values from the search space\n",
        "  num_hidden_layer = trial.suggest_int('num_hidden_layer',1,5)# search space is from 1 to 5\n",
        "  neuron_per_layer=trial.suggest_int('neuron_per_layer',8,128,step=8)#search space like 8-16-24-32..\n",
        "  epochs=trial.suggest_int('epochs',10,50,step=10)\n",
        "  learning_rate=trial.suggest_float('learning_rate',1e-5,1e-1,log=True)#logarithmically selection of learning_rate\n",
        "  dropout_rate=trial.suggest_float('dropout_rate',0.1,0.5,step=0.1)\n",
        "  #for implementing dropout we have to add new parameter in the MyNN class to use from here\n",
        "  batch_size=trial.suggest_categorical('batch_size',[16,32,64,128])#select from these list items\n",
        "  optimizer_name=trial.suggest_categorical('optimizer',['Adam','SGD','RMSprop'])#select from one of these  but we have to implement if-else statement for this.\n",
        "  weight_decay=trial.suggest_float('weight_decay',1e-5,1e-3,log=True)#logarithmic selection\n",
        "\n",
        "  #now the train_loader and test_loader will also be used inside the objective function because it can't be defined previously now.\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "  test_loader =  DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  #model init\n",
        "  input_dim = 784\n",
        "  output_dim = 10\n",
        "\n",
        "  model = MyNN(input_dim,output_dim,num_hidden_layer,neuron_per_layer,dropout_rate)\n",
        "  model.to(device)#directed to GPU\n",
        "\n",
        "  #param init\n",
        "  # learning_rate=0.01\n",
        "  # epochs=50\n",
        "\n",
        "\n",
        "  #optimizer selection\n",
        "  criterion=nn.CrossEntropyLoss()\n",
        "\n",
        "  if optimizer_name=='Adam':\n",
        "    optimimzer=optim.Adam(model.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
        "  elif optimizer_name=='SGD':\n",
        "    optimimzer=optim.SGD(model.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
        "  else :\n",
        "    optimimzer=optim.RMSprop(model.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
        "\n",
        "  #training loop\n",
        "  total_epoch_loss = 0.0 # Initialize total_epoch_loss before the loop\n",
        "  for epoch in range(epochs):\n",
        "      for batch_features,batch_labels in train_loader:\n",
        "            # Flatten the image data\n",
        "            batch_features = batch_features.view(-1, 28 * 28).to(device) # Move data to device\n",
        "            batch_labels = batch_labels.to(device) # Move labels to device\n",
        "\n",
        "            #forward pass\n",
        "            outputs=model(batch_features)\n",
        "            #loss calculate\n",
        "            loss=criterion(outputs,batch_labels)\n",
        "            #gradient zero\n",
        "            optimimzer.zero_grad() # Corrected optimizer name\n",
        "            #back propogation\n",
        "            loss.backward()\n",
        "            #upgrade gradients\n",
        "            optimimzer.step() # Corrected optimizer name\n",
        "\n",
        "    #         total_epoch_loss += loss.item() # Accumulate loss\n",
        "\n",
        "    # avg_loss=total_epoch_loss/len(train_loader) #avg epoch loss\n",
        "    # print(f\"epoch : {epoch+1}, Loss : {avg_loss}\")\n",
        "\n",
        "  #evaluation\n",
        "  #set model to eval mode\n",
        "  model.eval()\n",
        "  # evaluation code for test data :\n",
        "  total=0\n",
        "  correct=0\n",
        "  with torch.no_grad():\n",
        "    for batch_features,batch_labels in test_loader:\n",
        "      # Flatten the image data\n",
        "      batch_features = batch_features.view(-1, 28 * 28).to(device) # Move data to device\n",
        "      batch_labels = batch_labels.to(device) # Move labels to device\n",
        "\n",
        "      outputs=model(batch_features)\n",
        "      _,predicted=torch.max(outputs.data,1)\n",
        "      total+=batch_labels.shape[0]\n",
        "      correct+=(predicted==batch_labels).sum().item()\n",
        "    accuracy=correct/total*100\n",
        "\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "O2J9FwbLKjQX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATING STUDY USING OPTUNA"
      ],
      "metadata": {
        "id": "OwuUUnZfKnGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRkFxW5lKmSL",
        "outputId": "d07100db-c9c1-41e2-90f7-b2a9ff141eb2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.16.5)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.9.0 optuna-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "study = optuna.create_study(direction='maximize')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4WVowQbKuYx",
        "outputId": "58832b0b-3549-470d-c120-66e3dac0cabd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-09-12 04:24:31,687] A new study created in memory with name: no-name-d1fc4286-e073-4bce-b8a4-268e19cdbc5b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.optimize(objective,n_trials=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM32newnKw3I",
        "outputId": "7b6e8586-7f9a-46cb-d254-db5f3d37ac1a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-09-12 04:26:13,288] Trial 0 finished with value: 95.59 and parameters: {'num_hidden_layer': 2, 'neuron_per_layer': 40, 'epochs': 20, 'learning_rate': 0.0030967003391325536, 'dropout_rate': 0.4, 'batch_size': 32, 'optimizer': 'Adam', 'weight_decay': 0.00013551565383828953}. Best is trial 0 with value: 95.59.\n",
            "[I 2025-09-12 04:27:14,076] Trial 1 finished with value: 96.46000000000001 and parameters: {'num_hidden_layer': 1, 'neuron_per_layer': 48, 'epochs': 30, 'learning_rate': 0.00014314655528781272, 'dropout_rate': 0.30000000000000004, 'batch_size': 64, 'optimizer': 'Adam', 'weight_decay': 1.5526910461359215e-05}. Best is trial 1 with value: 96.46000000000001.\n",
            "[I 2025-09-12 04:28:48,548] Trial 2 finished with value: 97.6 and parameters: {'num_hidden_layer': 4, 'neuron_per_layer': 72, 'epochs': 50, 'learning_rate': 2.4799410454874547e-05, 'dropout_rate': 0.1, 'batch_size': 128, 'optimizer': 'Adam', 'weight_decay': 0.0005133146513986134}. Best is trial 2 with value: 97.6.\n",
            "[I 2025-09-12 04:29:23,422] Trial 3 finished with value: 94.0 and parameters: {'num_hidden_layer': 4, 'neuron_per_layer': 72, 'epochs': 20, 'learning_rate': 0.0013718305422448188, 'dropout_rate': 0.2, 'batch_size': 128, 'optimizer': 'SGD', 'weight_decay': 0.0005640799400408826}. Best is trial 2 with value: 97.6.\n",
            "[I 2025-09-12 04:31:49,410] Trial 4 finished with value: 96.86 and parameters: {'num_hidden_layer': 3, 'neuron_per_layer': 64, 'epochs': 50, 'learning_rate': 1.9496943867996366e-05, 'dropout_rate': 0.2, 'batch_size': 64, 'optimizer': 'Adam', 'weight_decay': 0.00035137538483718205}. Best is trial 2 with value: 97.6.\n",
            "[I 2025-09-12 04:34:29,338] Trial 5 finished with value: 94.82000000000001 and parameters: {'num_hidden_layer': 1, 'neuron_per_layer': 88, 'epochs': 50, 'learning_rate': 0.0006697266049954235, 'dropout_rate': 0.5, 'batch_size': 32, 'optimizer': 'SGD', 'weight_decay': 1.1732419208624296e-05}. Best is trial 2 with value: 97.6.\n",
            "[I 2025-09-12 04:37:59,257] Trial 6 finished with value: 91.79 and parameters: {'num_hidden_layer': 5, 'neuron_per_layer': 80, 'epochs': 30, 'learning_rate': 0.04535776957682152, 'dropout_rate': 0.30000000000000004, 'batch_size': 32, 'optimizer': 'Adam', 'weight_decay': 2.0278895671647328e-05}. Best is trial 2 with value: 97.6.\n",
            "[I 2025-09-12 04:39:03,844] Trial 7 finished with value: 58.989999999999995 and parameters: {'num_hidden_layer': 5, 'neuron_per_layer': 88, 'epochs': 10, 'learning_rate': 0.0009273390768651148, 'dropout_rate': 0.5, 'batch_size': 32, 'optimizer': 'SGD', 'weight_decay': 2.2720859223614505e-05}. Best is trial 2 with value: 97.6.\n",
            "[I 2025-09-12 04:40:28,200] Trial 8 finished with value: 93.19 and parameters: {'num_hidden_layer': 2, 'neuron_per_layer': 24, 'epochs': 10, 'learning_rate': 0.003797770602888332, 'dropout_rate': 0.4, 'batch_size': 16, 'optimizer': 'Adam', 'weight_decay': 1.128395808707413e-05}. Best is trial 2 with value: 97.6.\n",
            "[I 2025-09-12 04:48:00,446] Trial 9 finished with value: 93.89 and parameters: {'num_hidden_layer': 3, 'neuron_per_layer': 32, 'epochs': 50, 'learning_rate': 0.0015577884954435523, 'dropout_rate': 0.4, 'batch_size': 16, 'optimizer': 'SGD', 'weight_decay': 0.0006962299029867182}. Best is trial 2 with value: 97.6.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(study.best_value)\n",
        "print(study.best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0igNk8quK_PB",
        "outputId": "be64c28a-42c6-4873-ffba-d18f87dc7e27"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97.6\n",
            "{'num_hidden_layer': 4, 'neuron_per_layer': 72, 'epochs': 50, 'learning_rate': 2.4799410454874547e-05, 'dropout_rate': 0.1, 'batch_size': 128, 'optimizer': 'Adam', 'weight_decay': 0.0005133146513986134}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPLEMENTING MODEL"
      ],
      "metadata": {
        "id": "li6K7UfeQQLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best parameters from the study\n",
        "best_params = study.best_params\n",
        "\n",
        "# Extract the best parameters for the model\n",
        "num_hidden_layer = best_params['num_hidden_layer']\n",
        "neuron_per_layer = best_params['neuron_per_layer']\n",
        "dropout_rate = best_params['dropout_rate']\n",
        "\n",
        "# Define input and output dimensions\n",
        "input_dim = 784 # For flattened 28x28 MNIST images\n",
        "output_dim = 10 # For 10 digits (0-9)\n",
        "\n",
        "# Create the model with the best parameters\n",
        "model = MyNN(input_dim, output_dim, num_hidden_layer, neuron_per_layer, dropout_rate)\n",
        "\n",
        "# Move the model to the appropriate device (GPU if available)\n",
        "model.to(device)\n",
        "\n",
        "print(\"Model created with best hyperparameters:\")\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_WORg-aQTkk",
        "outputId": "cbe69550-be71-4e14-8a40-26c3d612845e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created with best hyperparameters:\n",
            "MyNN(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=72, bias=True)\n",
            "    (1): BatchNorm1d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Linear(in_features=72, out_features=72, bias=True)\n",
            "    (5): BatchNorm1d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): Dropout(p=0.1, inplace=False)\n",
            "    (8): Linear(in_features=72, out_features=72, bias=True)\n",
            "    (9): BatchNorm1d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU()\n",
            "    (11): Dropout(p=0.1, inplace=False)\n",
            "    (12): Linear(in_features=72, out_features=72, bias=True)\n",
            "    (13): BatchNorm1d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (14): ReLU()\n",
            "    (15): Dropout(p=0.1, inplace=False)\n",
            "    (16): Linear(in_features=72, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0415f6f",
        "outputId": "402bc2d4-f70f-4841-f570-c488d6d72792"
      },
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Get optimizer from best parameters\n",
        "optimizer_name = best_params['optimizer']\n",
        "learning_rate = best_params['learning_rate']\n",
        "weight_decay = best_params['weight_decay']\n",
        "\n",
        "if optimizer_name == 'Adam':\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "elif optimizer_name == 'SGD':\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "else: # RMSprop\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# Get the number of epochs and batch size from best parameters\n",
        "epochs = best_params['epochs']\n",
        "batch_size = best_params['batch_size']\n",
        "\n",
        "# Re-create DataLoaders with the best batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(epochs):\n",
        "    model.train() # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        # Flatten the input images\n",
        "        inputs = inputs.view(inputs.size(0), -1)\n",
        "\n",
        "        # Move inputs and labels to the device\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print epoch loss\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Training finished.\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/50, Loss: 1.9102\n",
            "Epoch 2/50, Loss: 1.2983\n",
            "Epoch 3/50, Loss: 0.9607\n",
            "Epoch 4/50, Loss: 0.7453\n",
            "Epoch 5/50, Loss: 0.6048\n",
            "Epoch 6/50, Loss: 0.5050\n",
            "Epoch 7/50, Loss: 0.4371\n",
            "Epoch 8/50, Loss: 0.3831\n",
            "Epoch 9/50, Loss: 0.3427\n",
            "Epoch 10/50, Loss: 0.3141\n",
            "Epoch 11/50, Loss: 0.2884\n",
            "Epoch 12/50, Loss: 0.2672\n",
            "Epoch 13/50, Loss: 0.2518\n",
            "Epoch 14/50, Loss: 0.2343\n",
            "Epoch 15/50, Loss: 0.2293\n",
            "Epoch 16/50, Loss: 0.2152\n",
            "Epoch 17/50, Loss: 0.2054\n",
            "Epoch 18/50, Loss: 0.1952\n",
            "Epoch 19/50, Loss: 0.1916\n",
            "Epoch 20/50, Loss: 0.1839\n",
            "Epoch 21/50, Loss: 0.1788\n",
            "Epoch 22/50, Loss: 0.1744\n",
            "Epoch 23/50, Loss: 0.1700\n",
            "Epoch 24/50, Loss: 0.1648\n",
            "Epoch 25/50, Loss: 0.1610\n",
            "Epoch 26/50, Loss: 0.1587\n",
            "Epoch 27/50, Loss: 0.1542\n",
            "Epoch 28/50, Loss: 0.1509\n",
            "Epoch 29/50, Loss: 0.1463\n",
            "Epoch 30/50, Loss: 0.1448\n",
            "Epoch 31/50, Loss: 0.1409\n",
            "Epoch 32/50, Loss: 0.1381\n",
            "Epoch 33/50, Loss: 0.1378\n",
            "Epoch 34/50, Loss: 0.1367\n",
            "Epoch 35/50, Loss: 0.1313\n",
            "Epoch 36/50, Loss: 0.1304\n",
            "Epoch 37/50, Loss: 0.1271\n",
            "Epoch 38/50, Loss: 0.1256\n",
            "Epoch 39/50, Loss: 0.1247\n",
            "Epoch 40/50, Loss: 0.1239\n",
            "Epoch 41/50, Loss: 0.1212\n",
            "Epoch 42/50, Loss: 0.1212\n",
            "Epoch 43/50, Loss: 0.1154\n",
            "Epoch 44/50, Loss: 0.1139\n",
            "Epoch 45/50, Loss: 0.1138\n",
            "Epoch 46/50, Loss: 0.1116\n",
            "Epoch 47/50, Loss: 0.1113\n",
            "Epoch 48/50, Loss: 0.1122\n",
            "Epoch 49/50, Loss: 0.1082\n",
            "Epoch 50/50, Loss: 0.1070\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUATION"
      ],
      "metadata": {
        "id": "YcNwZQ7TEwMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation\n",
        "#set model to eval mode\n",
        "model.eval()\n",
        "# evaluation code for test data :\n",
        "total=0\n",
        "correct=0\n",
        "with torch.no_grad():\n",
        "  for batch_features,batch_labels in test_loader:\n",
        "    # Flatten the image data\n",
        "    batch_features = batch_features.view(batch_features.size(0), -1)\n",
        "    #move data to gpu\n",
        "    batch_features = batch_features.to(device)\n",
        "    batch_labels = batch_labels.to(device)\n",
        "\n",
        "    outputs=model(batch_features)\n",
        "    _,predicted=torch.max(outputs.data,1)\n",
        "    total+=batch_labels.shape[0]\n",
        "    correct+=(predicted==batch_labels).sum().item()\n",
        "  accuracy=correct/total*100\n",
        "\n",
        "print(f\"Accuracy on test set: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIsIGY2EEx_T",
        "outputId": "440799f6-b36a-4e05-da88-cc49091403c2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 97.45%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation\n",
        "#set model to eval mode\n",
        "model.eval()\n",
        "# evaluation code for train data :\n",
        "total=0\n",
        "correct=0\n",
        "with torch.no_grad():\n",
        "  for batch_features,batch_labels in train_loader:\n",
        "    # Flatten the image data\n",
        "    batch_features = batch_features.view(batch_features.size(0), -1)\n",
        "    #move data to gpu\n",
        "    batch_features = batch_features.to(device)\n",
        "    batch_labels = batch_labels.to(device)\n",
        "\n",
        "    outputs=model(batch_features)\n",
        "    _,predicted=torch.max(outputs.data,1)\n",
        "    total+=batch_labels.shape[0]\n",
        "    correct+=(predicted==batch_labels).sum().item()\n",
        "  accuracy=correct/total*100\n",
        "\n",
        "print(f\"Accuracy on train set: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W82zNx9KAMI",
        "outputId": "1e9c9c62-3000-4832-d085-ffbc0d81b0d0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on train set: 98.73%\n"
          ]
        }
      ]
    }
  ]
}